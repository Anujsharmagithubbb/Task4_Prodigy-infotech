{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyPGvEd358NRLrUf5ncNv6bD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anujsharmagithubbb/Task4_Prodigy-infotech/blob/main/Task4_Prodigy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# -----------------------------\n",
        "# Config\n",
        "# -----------------------------\n",
        "class Config:\n",
        "    dataset_root = \"./data/facades\"  # contains 'train' and 'val' folders\n",
        "    batch_size = 8\n",
        "    image_size = 256\n",
        "    epochs = 200\n",
        "    lr = 2e-4\n",
        "    beta1 = 0.5\n",
        "    lambda_l1 = 100.0\n",
        "    checkpoint_dir = \"./checkpoints\"\n",
        "    sample_dir = \"./samples\"\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    num_workers = 2\n",
        "    save_every = 5\n",
        "\n",
        "cfg = Config()\n",
        "\n",
        "os.makedirs(cfg.checkpoint_dir, exist_ok=True)\n",
        "os.makedirs(cfg.sample_dir, exist_ok=True)\n",
        "\n",
        "# -----------------------------\n",
        "# Dataset loader\n",
        "# -----------------------------\n",
        "class PairedImageDataset(Dataset):\n",
        "    def __init__(self, root: str, mode: str = \"train\", image_size: int = 256):\n",
        "        self.dir = Path(root) / mode\n",
        "        self.files = sorted([p for p in self.dir.iterdir() if p.suffix.lower() in {'.jpg', '.png', '.jpeg'}])\n",
        "        self.size = image_size\n",
        "\n",
        "        self.transform = transforms.Compose([\n",
        "            transforms.Resize((self.size, self.size)),\n",
        "            transforms.ToTensor(),\n",
        "        ])\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)\n",
        "\n",
        "    def __getitem__(self, idx: int):\n",
        "        path = self.files[idx]\n",
        "        img = Image.open(path).convert('RGB')\n",
        "        w, h = img.size\n",
        "        w2 = w // 2\n",
        "        A = img.crop((0, 0, w2, h))\n",
        "        B = img.crop((w2, 0, w, h))\n",
        "\n",
        "        A = self.transform(A)\n",
        "        B = self.transform(B)\n",
        "\n",
        "        A = (A - 0.5) * 2.0\n",
        "        B = (B - 0.5) * 2.0\n",
        "\n",
        "        return {'A': A, 'B': B, 'path': str(path)}\n",
        "\n",
        "# -----------------------------\n",
        "# Network blocks\n",
        "# -----------------------------\n",
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        nn.init.constant_(m.bias.data, 0)\n",
        "\n",
        "\n",
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, normalize=True, dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers = [nn.Conv2d(in_channels, out_channels, 4, 2, 1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm2d(out_channels))\n",
        "        layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, dropout=0.0):\n",
        "        super().__init__()\n",
        "        layers = [\n",
        "            nn.ConvTranspose2d(in_channels, out_channels, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(out_channels),\n",
        "            nn.ReLU(inplace=True)\n",
        "        ]\n",
        "        if dropout:\n",
        "            layers.append(nn.Dropout(dropout))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.model(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "6CJbHC3eYHoc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}